{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using the Tie Kernel\n",
    "\n",
    "This kernel allows us to tie together the values of various parameters.\n",
    "\n",
    "## Example 1: Simple RBF kernel with equal lengthscale and variance\n",
    "\n",
    "Bit of an odd example, but simple to start with, we fix the lengthscale equal to the variance of the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import GPy\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "\n",
    "X = np.arange(20)[:,None]\n",
    "Y = np.sin(X)\n",
    "simple = GPy.kern.RBF(input_dim=1,lengthscale=2.0,variance=1.0)\n",
    "\n",
    "tiekern = GPy.kern.Tie(simple,1,[['.*']])\n",
    "tietest = GPy.models.GPRegression(X,Y,tiekern)\n",
    "tietest.checkgrad(verbose=1)\n",
    "#tietest.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that the lengthscale and variance are now no longer fulfilling the gradient check. I've maybe not thought this through properly, and just took Max's suggestion to add together the gradients. We have a log likelihood, $L=log p(y|X,l,\\sigma)$ and we know $\\frac{\\delta L}{\\delta l}$ and $\\frac{\\delta L}{\\delta \\sigma}$. We are now fixing these to be equal. If we consider the infinitesimal movement from $l$ to $l+\\Delta l$ and from $\\sigma$ to $\\sigma + \\Delta \\sigma$ the two $\\Delta$ terms are equal. If the gradient wrt $\\sigma$ at $l$ is equal to the gradient at $l + \\Delta l$ then we can say that the change in $L$ between $(l,\\sigma)$ is equal to the change that occurs along $l$ plus the change that occurs along $\\sigma$, the upshot is we can add the two gradients together.\n",
    "\n",
    "To achieve this in the kernel, I compute the new analytical values for the gradients, find their mean, and set all the gradients to this mean. \n",
    "\n",
    "One problem that appears above is the numerical approximation is calculated by adding a $\\Delta l$ to the lengthscale, while the *analytical* one is effectively what happens if you add that delta to the variance too.\n",
    "\n",
    "Below we demonstrate that by looking at how the likelihood changes when you move in both directions at the same time,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.sum(tietest.gradient[0:2])\n",
    "before = tietest.log_likelihood()\n",
    "tietest.tie.rbf.variance+=0.00001\n",
    "tietest.tie.rbf.lengthscale+=0.00001\n",
    "after = tietest.log_likelihood()\n",
    "(after-before)/0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the values of the kernel after optimizing, included to show that the two parameters are equal to each other now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tietest.optimize()\n",
    "tietest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second example: For clustering\n",
    "\n",
    "This is the motivation for this, I wanted to fit a set of models using the independent outputs kernel and the offset kernel.\n",
    "\n",
    "Specifically I take two datasets and want to know whether they are better fitted together or seperately. Is the log likelihood increased by combining them?\n",
    "\n",
    "On the outside is the 'Tie' kernel, which allows us to tie together different parameters.\n",
    "Within this is the independent outputs kernel, this allows us to have three seperate kernels running together: The first is the combined 'Offset' kernel, which takes the two datasets and allows them to shift against one another. The other two are the datasets fitted individually.\n",
    "\n",
    "    Tie[ MultipleOutput( Offset{ Common }, Common, Common ) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import GPy\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "\n",
    "N = 10\n",
    "X1 = np.arange(1,N+1,1)[:,None]\n",
    "X2 = X1 + 1.2\n",
    "X = np.vstack([X1,X2,X1,X2])\n",
    "nans = np.empty([N*2,1])\n",
    "nans[:] = np.NAN\n",
    "\n",
    "#offset indicies\n",
    "ind_offset = np.vstack([np.zeros([N,1]),np.ones([N,1]),nans])\n",
    "\n",
    "#independent output indicies\n",
    "ind_indpoutputs = np.vstack([np.zeros([N*2,1]),np.ones([N,1]),np.ones([N,1])*2])\n",
    "X = np.hstack([X,ind_offset,ind_indpoutputs])\n",
    "Y1 = np.sin((X[0:N,0])/10.0)[:,None]\n",
    "#Y2 = np.sin((X[0:N,0])/10.0)[:,None]\n",
    "Y2 = np.cos((X[0:N,0])/10.0)[:,None]\n",
    "Y1 += np.random.randn(Y1.shape[0],Y1.shape[1])*0.1\n",
    "Y2 += np.random.randn(Y2.shape[0],Y2.shape[1])*0.1\n",
    "Y = np.vstack([Y1,Y2,Y1,Y2])\n",
    "\n",
    "#Structure of inputs:\n",
    "# actual input | offset_kernel_index | indp_output_index\n",
    "#      2.4              0                     0\n",
    "#      2.9              0                     0\n",
    "#      3.4              1                     0\n",
    "#      3.9              1                     0\n",
    "#      2.4              nan                   1\n",
    "#      2.9              nan                   1\n",
    "#      3.4              nan                   2\n",
    "#      3.9              nan                   2\n",
    "#print X\n",
    "#print Y\n",
    "\n",
    "#base kernel to explain all time series with\n",
    "common_kern = GPy.kern.Matern32(input_dim=1)\n",
    "\n",
    "#the offset kernel, that can shift one time series wrt another\n",
    "offset_kern = GPy.kern.Offset(common_kern,2,[0])\n",
    "\n",
    "#we want to discourage massive offsets, which can achieve good fits by simply moving the two datasets far apart\n",
    "offset_kern.offset.set_prior(GPy.priors.Gaussian(0,4.0))\n",
    "\n",
    "#our overall kernel contains our offset kernel and two common kernels\n",
    "independent_kern = GPy.kern.IndependentOutputs([offset_kern,common_kern.copy(),common_kern.copy()],index_dim=2)\n",
    "\n",
    "tiekern = GPy.kern.Tie(independent_kern,3,[['.*lengthscale'],['.*variance']])\n",
    "\n",
    "model = GPy.models.GPRegression(X,Y,tiekern)\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two datasets combined, with the offset applied. The plot function below doesn't show the offset occuring, so I've quickly plotted the data offset instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.plot(fixed_inputs=[(1,0),(2,0)],plot_data=False)\n",
    "plt.plot(X2-model.tie.independ.offset.offset.values[0],Y2[0:10],'kx')\n",
    "plt.plot(X1,Y1[0:10],'kx')\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below is for the first single dataset (so ignore the unfitted rightward points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.plot(fixed_inputs=[(1,0),(2,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below is for the second single dataset (so ignore the unfitted leftward points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.plot(fixed_inputs=[(1,0),(2,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The X matrix:\n",
    "#\n",
    "# actual input | offset_kernel_index | indp_output_index\n",
    "#      2.4              0                     0\n",
    "#      2.9              0                     0\n",
    "#      3.4              1                     0\n",
    "#      3.9              1                     0\n",
    "#      2.4              nan                   1\n",
    "#      2.9              nan                   1\n",
    "#      3.4              nan                   2\n",
    "#      3.9              nan                   2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the above parameters and fit them separately in our own models. We can then get the loglikelihoods and compare if the sum of the individual models is greater or lesser than the combined model.\n",
    "\n",
    "Notice that the hyperparameters will all now be equal, so the model complexity should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(X)\n",
    "indepY = Y[n/2:]\n",
    "indepX = X[n/2:,[0,2]]\n",
    "indepX[:,1]-=1\n",
    "offsetY = Y[:n/2]\n",
    "offsetX = X[:n/2,[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base kernel to explain all time series with\n",
    "common_kern = GPy.kern.Matern32(input_dim=1)\n",
    "\n",
    "#the offset kernel, that can shift one time series wrt another\n",
    "offset_kern = GPy.kern.Offset(common_kern,2,[0])\n",
    "\n",
    "#we want to discourage massive offsets, which can achieve good fits by simply moving the two datasets far apart\n",
    "offset_kern.offset.set_prior(GPy.priors.Gaussian(0,4.0))\n",
    "\n",
    "#our overall kernel contains our offset kernel and two common kernels\n",
    "independent_kern = GPy.kern.IndependentOutputs([common_kern.copy(),common_kern.copy()],index_dim=1)\n",
    "independent_model = GPy.models.GPRegression(indepX,indepY,independent_kern)\n",
    "\n",
    "offset_model = GPy.models.GPRegression(offsetX,offsetY,offset_kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "independent_model.kern.Mat32.variance.fix(model.kern.independ.Mat32.variance[:])\n",
    "independent_model.kern.Mat32_1.variance.fix(model.kern.independ.Mat32_1.variance[:])\n",
    "independent_model.kern.Mat32.lengthscale.fix(model.kern.independ.Mat32.lengthscale[:])\n",
    "independent_model.kern.Mat32_1.lengthscale.fix(model.kern.independ.Mat32_1.lengthscale[:])\n",
    "independent_model.Gaussian_noise.fix(model.Gaussian_noise[:])\n",
    "\n",
    "independent_model.update_toggle = True\n",
    "independent_model.trigger_update()\n",
    "\n",
    "offset_model.kern.Mat32.variance.fix(model.kern.independ.offset.Mat32.variance[:])\n",
    "offset_model.kern.Mat32.lengthscale.fix(model.kern.independ.offset.Mat32.lengthscale[:])\n",
    "offset_model.Gaussian_noise.fix(model.Gaussian_noise[:])\n",
    "offset_model.kern.offset.fix(model.kern.independ.offset.offset[:])\n",
    "\n",
    "offset_model.trigger_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.plot(fixed_inputs=[(1,0),(2,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offset_model.plot(fixed_inputs=[(1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Just checking that the independent model's likelihood is the sum of the two\n",
    "#models it's made of\n",
    "\n",
    "simple_model1 = GPy.models.GPRegression(X1,Y1,common_kern)\n",
    "simple_model1.Gaussian_noise.fix(model.Gaussian_noise[:])\n",
    "simple_model1.kern.variance.fix(model.kern.independ.Mat32.variance[:])\n",
    "simple_model1.kern.lengthscale.fix(model.kern.independ.Mat32.lengthscale[:])\n",
    "simple_model1.update_toggle = True\n",
    "simple_model1.update_model()\n",
    "simple_model1.plot()\n",
    "\n",
    "simple_model2 = GPy.models.GPRegression(X2,Y2,common_kern)\n",
    "simple_model2.Gaussian_noise.fix(model.Gaussian_noise[:])\n",
    "simple_model2.kern.variance.fix(model.kern.independ.Mat32.variance[:])\n",
    "simple_model2.kern.lengthscale.fix(model.kern.independ.Mat32.lengthscale[:])\n",
    "simple_model2.update_toggle = True\n",
    "simple_model2.update_model()\n",
    "print simple_model1.log_likelihood() + simple_model2.log_likelihood()\n",
    "print independent_model.log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offset_model.log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "active = []\n",
    "for p in range(0,N):\n",
    "    active.append([p])\n",
    "\n",
    "diffloglikes = np.zeros([len(active),len(active)])\n",
    "diffloglikes[:] = None\n",
    "offsets = np.zeros([len(active),len(active)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clusti in range(len(active)):        \n",
    "    for clustj in range(clusti): #count from 0 to clustj-1\n",
    "        if np.isnan(diffloglikes[clusti,clustj]):\n",
    "            diffloglikes[clusti,clustj] = np.random.rand()\n",
    "            offsets[clusti,clustj] = np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top = np.unravel_index(np.nanargmax(diffloglikes), diffloglikes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diffloglikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "data = []\n",
    "for n in range(N):\n",
    "    inputs.append(np.random.randn(3,1))\n",
    "    data.append(np.random.randn(1,3))\n",
    "if diffloglikes[top[0],top[1]]>0:\n",
    "    active[top[0]].extend(active[top[1]])\n",
    "    offset=offsets[top[0],top[1]]\n",
    "    inputs[top[0]] = np.vstack([inputs[top[0]],inputs[top[1]]-offset])\n",
    "    data[top[0]] = np.hstack([data[top[0]],data[top[1]]])\n",
    "    del inputs[top[1]]\n",
    "    del data[top[1]]\n",
    "    del active[top[1]]\n",
    "    \n",
    "    #None = we need to recalculate\n",
    "    diffloglikes[:,top[0]] = None\n",
    "    diffloglikes[top[0],:] = None\n",
    "    diffloglikes = np.delete(diffloglikes,top[1],0)\n",
    "    diffloglikes = np.delete(diffloglikes,top[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diffloglikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array([[      nan,     nan,     nan,     nan],\n",
    "       [,     nan,     nan,     nan,     nan],\n",
    "       [ ,  0.1765,     nan,     nan,     nan],\n",
    "               ?         ?      ?          ?\n",
    "       [   0.0623,  0.0079,  0.4415,     nan]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining into a single python method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import GPy\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "\n",
    "\n",
    "def get_log_likelihood_diff(inputs,data,clusti,clustj,common_kern):\n",
    "    data_i = data[clusti]\n",
    "    data_j = data[clustj]\n",
    "    inputs_i = inputs[clusti]\n",
    "    inputs_j = inputs[clustj]\n",
    "    Ni = data_i.shape[0]\n",
    "    Nj = data_j.shape[0]\n",
    "    N = Ni+Nj\n",
    "#    print \"CLUSTI, CLUSTJ\"\n",
    "#    print clusti, clustj\n",
    "#    print \"NI, NJ\"\n",
    "#    print Ni, Nj\n",
    "#    print \"INPUTS_I SHAPE\"\n",
    "#    print inputs_i.shape\n",
    "#    print \"INPUTS_J SHAPE\"\n",
    "#    print inputs_j.shape\n",
    "    X = np.vstack([inputs_i,inputs_j,inputs_i,inputs_j])\n",
    "    nans = np.empty([N,1])\n",
    "    nans[:] = np.NAN\n",
    "    \n",
    "    #offset indicies\n",
    "    ind_offset = np.vstack([np.zeros([Ni,1]),np.ones([Nj,1]),nans])\n",
    "\n",
    "    #independent output indicies\n",
    "    ind_indpoutputs = np.vstack([np.zeros([N,1]),np.ones([Ni,1]),np.ones([Nj,1])*2])\n",
    "    X = np.hstack([X,ind_offset,ind_indpoutputs])\n",
    "    Y1 = data_i #[:,None]\n",
    "    Y2 = data_j #[:,None]\n",
    "    Y = np.vstack([Y1,Y2,Y1,Y2])\n",
    "\n",
    "    \n",
    "    #these are actually the same, could equate later TODO\n",
    "    indepY = np.vstack([Y1,Y2])\n",
    "    indepX = np.hstack([np.vstack([inputs_i,inputs_j]),np.vstack([np.zeros([Ni,1]),np.ones([Nj,1])])])\n",
    "    \n",
    "    offsetY = np.vstack([Y1,Y2])\n",
    "    offsetX = np.hstack([np.vstack([inputs_i,inputs_j]),np.vstack([np.zeros([Ni,1]),np.ones([Nj,1])])]) \n",
    "    \n",
    "    #Structure of inputs:\n",
    "    # actual input | offset_kernel_index | indp_output_index\n",
    "    #      2.4              0                     0\n",
    "    #      2.9              0                     0\n",
    "    #      3.4              1                     0\n",
    "    #      3.9              1                     0\n",
    "    #      2.4              nan                   1\n",
    "    #      2.9              nan                   1\n",
    "    #      3.4              nan                   2\n",
    "    #      3.9              nan                   2\n",
    "    #print X\n",
    "    #print Y\n",
    "\n",
    "    #base kernel to explain all time series with\n",
    "    common_kern = GPy.kern.Matern32(input_dim=1)\n",
    "\n",
    "    #the offset kernel, that can shift one time series wrt another\n",
    "    offset_kern = GPy.kern.Offset(common_kern,2,[0])\n",
    "\n",
    "    #we want to discourage massive offsets, which can achieve good fits by simply moving the two datasets far apart\n",
    "    offset_kern.offset.set_prior(GPy.priors.Gaussian(0,4.0))\n",
    "\n",
    "    #our overall kernel contains our offset kernel and two common kernels\n",
    "    independent_kern = GPy.kern.IndependentOutputs([offset_kern,common_kern.copy(),common_kern.copy()],index_dim=2)\n",
    "\n",
    "    tiekern = GPy.kern.Tie(independent_kern,3,[['.*lengthscale'],['.*variance']])\n",
    "    model = GPy.models.GPRegression(X,Y,tiekern)\n",
    "    \n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    #the offset kernel, that can shift one time series wrt another\n",
    "    offset_kern = GPy.kern.Offset(common_kern,2,[0])\n",
    "\n",
    "    #we want to discourage massive offsets, which can achieve good fits by simply moving the two datasets far apart\n",
    "    offset_kern.offset.set_prior(GPy.priors.Gaussian(0,4.0))\n",
    "\n",
    "    #our overall kernel contains our offset kernel and two common kernels\n",
    "    independent_kern = GPy.kern.IndependentOutputs([common_kern.copy(),common_kern.copy()],index_dim=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    independent_model = GPy.models.GPRegression(indepX,indepY,independent_kern)\n",
    "\n",
    "    offset_model = GPy.models.GPRegression(offsetX,offsetY,offset_kern)\n",
    "    offset = offset_model.offset[0]\n",
    "    return offset_model.log_likelihood()-independent_model.log_likelihood(), offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster(data,inputs,common_kern):\n",
    "    active = []\n",
    "    for p in range(0,N):\n",
    "        active.append([p])\n",
    "    diffloglikes = np.zeros([len(active),len(active)])\n",
    "    diffloglikes[:] = None\n",
    "    offsets = np.zeros([len(active),len(active)])\n",
    "    while True:\n",
    "        for clusti in range(len(active)):        \n",
    "            for clustj in range(clusti): #count from 0 to clustj-1\n",
    "                if np.isnan(diffloglikes[clusti,clustj]):\n",
    "                    diffloglikes[clusti,clustj],offsets[clusti,clustj] = get_log_likelihood_diff(inputs,data,clusti,clustj,common_kern)\n",
    "\n",
    "        top = np.unravel_index(np.nanargmax(diffloglikes), diffloglikes.shape)\n",
    "\n",
    "        if diffloglikes[top[0],top[1]]>0:\n",
    "\n",
    "\n",
    "            active[top[0]].extend(active[top[1]])\n",
    "            offset=offsets[top[0],top[1]]\n",
    "            inputs[top[0]] = np.vstack([inputs[top[0]],inputs[top[1]]-offset])\n",
    "            data[top[0]] = np.vstack([data[top[0]],data[top[1]]])\n",
    "            del inputs[top[1]]\n",
    "            del data[top[1]]\n",
    "            del active[top[1]]\n",
    "\n",
    "            #None = we need to recalculate\n",
    "            diffloglikes[:,top[0]] = None\n",
    "            diffloglikes[top[0],:] = None\n",
    "            diffloglikes = np.delete(diffloglikes,top[1],0)\n",
    "            diffloglikes = np.delete(diffloglikes,top[1],1)\n",
    "        else:\n",
    "            break\n",
    "#        print \"ACTIVE\"\n",
    "#        print active\n",
    "#        print \"DIFF LOG LIKES\"\n",
    "#        print diffloglikes\n",
    "#        print \"DATA SHAPES\"\n",
    "#        for d in data:\n",
    "#            print d.shape\n",
    "    return active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 patients in current data set\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('~/GPy/GPy') #change to where your GPy is installed or remove if in path\n",
    "import GPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline\n",
    "#make some simulated data\n",
    "\n",
    "S = 9 #number of time series per data point\n",
    "\n",
    "#actual numbers of time points (0,1,2,3...)\n",
    "#Note I've already removed patients with less than 3 time points.\n",
    "Tpointcounts = np.array([   0.,    0.,    0.,  0.,   78.,   72.,   48.,   42.,   36., 32.,   23.,   11.,    5.,    5.,    6.,    4.,    9.,    3.,   3.,    4.,    2.,    1.,    2.,    1.,    3.,    1.,    2.,   1.,    1.,    2.,    0.,    0.,    1.,    0.,    1.,    0.,   1.,    0.,    0.])  \n",
    "Tpoints = []\n",
    "for i,count in enumerate(Tpointcounts):\n",
    "    for j in range(int(count)):\n",
    "        Tpoints.append(i)\n",
    "\n",
    "N = 12 #len(Tpoints) #we now get N from the number of points\n",
    "print(\"%d patients in current data set\" % N)\n",
    "C = 3  #number of clusters\n",
    "\n",
    "#actual latent function\n",
    "def lat_fn(t,s,c): #time, series, cluster\n",
    "    return np.sin(0.01*(1.0+t)*(10.0+c)/(10.0+c)+c)+t*(0.01*c*s)+c*0.2+1-s*0.02+s+c\n",
    "\n",
    "data = []\n",
    "inputs = []\n",
    "dataA = []\n",
    "inputsA = []\n",
    "groundtruth = []\n",
    "\n",
    "offsets = np.random.randn(N)*0 #no time offsets\n",
    "\n",
    "for p in range(N):\n",
    "    #sample from the known distribution of number of time points.\n",
    "    T = random.sample(Tpoints,1)[0]    \n",
    "    Tpoints.remove(T)\n",
    "\n",
    "    persondata = np.zeros([S,T])\n",
    "    personinputs = np.zeros([T,1])\n",
    "    indx = 0\n",
    "    c = np.random.randint(0,C) #pick cluster we're going to put person in\n",
    "    groundtruth.append(c)\n",
    "    pt = 0\n",
    "    for t in range(T):\n",
    "        personinputs[indx,0] = pt\n",
    "        indx+=1\n",
    "        for s in range(S):\n",
    "            persondata[s,t] = lat_fn(pt+offsets[p],s,c) + np.random.randn(1)*0.1\n",
    "        pt += np.random.randint(1,10)\n",
    "    data.append(persondata.T)\n",
    "    inputs.append(personinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6], [7, 5, 4, 0, 1], [11, 8, 2, 10, 9, 3]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_kern = GPy.kern.Matern32(input_dim=1)\n",
    "cluster(data,inputs,common_kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (array([ 2,  3,  8,  9, 10, 11]),)\n",
      "1 (array([6]),)\n",
      "2 (array([0, 1, 4, 5, 7]),)\n",
      "3 (array([], dtype=int64),)\n",
      "4 (array([], dtype=int64),)\n",
      "5 (array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "gt = np.array(groundtruth)\n",
    "for idx in range(6):\n",
    "    print idx,np.where(gt==idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
